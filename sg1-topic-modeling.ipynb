{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef76daf-a75f-4210-a9e9-713239fd09ec",
   "metadata": {},
   "source": [
    "# Topic Modeling of the television show Stargate SG1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bc576a-c156-435d-94fc-464a123d4e03",
   "metadata": {},
   "source": [
    "## Motivations\n",
    "\n",
    "I love the show Stargate SG1.  It is humorous with a level of self-awareness and does not take itself seriously, unlike some other sci-fi shows such as (Battlestar Gallactica or Stargate Universe).  Every year or so, I randomly re-watch the different seasons of the show.\n",
    "\n",
    "The show ran for 10 seasons with a total of 214 episodes.  It spanned multiple worlds as the SG1 team traveled and met with multiple alien races, some are allies, some are enemies.  It also had its own lores involving alien technologies, alien vocabularies, US military vocabularies, ancient Egyptian mythologies and pop-culture references.  \n",
    "\n",
    "Additionally, the episodes are generally self-contained, with the maximum that a storyline spans is around 2-3 episodes.  This makes the show easy to re-watch or to pick up at random spots without having to be updated of all the backgrounds and pertinent details.\n",
    "\n",
    "The motivation of this project is to build a corpus of all the transcripts of the 10 seasons, explore if there are recurrent themes and group the episodes and/or seasons by them.  By doing so, I'm looking to map the essential words for each theme and also the episodes relating to each theme.  This would allow one to select a group of theme or storyline and be able to watch all the episodes relating to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b58ce15-fb11-4498-8cb7-fc489d11b25d",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The following steps will be taken to explore this project:\n",
    "\n",
    "- Create Pipeline to scrape the transcript data\n",
    "- Preprocess and build a corpus of all the episodes\n",
    "- Tokenize and create a dictionary of all the words used in the episodes\n",
    "- Train and build embeddings from the tokens\n",
    "- Model the topics\n",
    "- Finally, explore the undercurrent themes and topics results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f43aff6-e683-4e07-b7dc-9629290d1836",
   "metadata": {},
   "source": [
    "### Data Source\n",
    "\n",
    "http://www.stargate-sg1-solutions.com/wiki/Transcripts appears to have the most complete transcripts for all 10 seasons.  The transcripts were compiled and archived by fans of the show.\n",
    "\n",
    "### Tools\n",
    "\n",
    "Methods / Tools to be used are:\n",
    "\n",
    "- Python\n",
    "- Beautifulsoup\n",
    "- Trafilatura\n",
    "- Gensim\n",
    "- Sklearn\n",
    "- Fasttext\n",
    "- LDA\n",
    "- HDBSCAN\n",
    "- BertTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67150c23-592a-4974-a7fd-bc279061bf6e",
   "metadata": {},
   "source": [
    "## Step 0: imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f048c083-7ec3-4321-bfdc-4039ad686a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import pathlib\n",
    "import pickle\n",
    "from typing import Iterator\n",
    "from tqdm.auto import trange, tqdm\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import gensim\n",
    "import spacy\n",
    "import sklearn\n",
    "import trafilatura\n",
    "from trafilatura import spider\n",
    "import courlan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f811347-3e80-46fc-bafc-c9b55fd478e2",
   "metadata": {},
   "source": [
    "## Step 1: Pipeline to Scrape Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f199978-5ba3-4efa-b5f1-1956952d634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript_links(urls: list) -> Iterator[set]:\n",
    "    \"\"\"\n",
    "    get all the links for each season's page\n",
    "    yield (generator) of sets\n",
    "    \"\"\"\n",
    "    for season in urls:\n",
    "        season_page = trafilatura.fetch_url(season, decode=True)\n",
    "        links = spider.extract_links(\n",
    "            pagecontent=season_page,\n",
    "            base_url=\"http://www.stargate-sg1-solutions.com\",\n",
    "            external_bool=False,\n",
    "        )\n",
    "        yield links\n",
    "\n",
    "\n",
    "def merge_uniqefy_links(links: Iterator[set]) -> set[str]:\n",
    "    \"\"\"\n",
    "    merge the list of sets\n",
    "    return a single set of unique links\n",
    "    \"\"\"\n",
    "    all_links = set()\n",
    "    for season in links:\n",
    "        all_links.update(season)\n",
    "\n",
    "    return all_links\n",
    "\n",
    "\n",
    "def filter_links(links: set[str]) -> Iterator:\n",
    "    \"\"\"\n",
    "    filter to only obtain the transcript links\n",
    "    transcript url is of pattern: \"wiki/[0-9].*Transcript\"\n",
    "    yield (generator) of links\n",
    "    \"\"\"\n",
    "    link_pattern = re.compile(r\"wiki\\/\\d.+Transcript$\")\n",
    "\n",
    "    for l in links:\n",
    "        if re.search(link_pattern, l):\n",
    "            yield l\n",
    "\n",
    "\n",
    "def extract_transcripts(filtered_links: set[str]) -> Iterator[dict]:\n",
    "    \"\"\"\n",
    "    extract the actual transcript from the set of transcript links\n",
    "    yield (generator) of dicts\n",
    "    \"\"\"\n",
    "    for l in filtered_links:\n",
    "        page = trafilatura.fetch_url(l)\n",
    "        extracted_page = trafilatura.bare_extraction(page)\n",
    "        extracted_transcript_list = extracted_page[\"text\"].split(\"\\n\")\n",
    "        start_index = extracted_transcript_list.index(\"Transcript\") + 1\n",
    "        for n, e in enumerate(extracted_transcript_list):\n",
    "            if re.search(r\"^Transcribed\", e):\n",
    "                end_index = n - 1\n",
    "        transcript_string = \" \".join(extracted_transcript_list[start_index:end_index])\n",
    "        extracted_page[\"text\"] = transcript_string\n",
    "        yield extracted_page\n",
    "\n",
    "\n",
    "def run_extraction_pipeline(urls: list) -> list[dict]:\n",
    "    # list of dicts\n",
    "    # with the actual transcript stored in the dict's \"text\" key\n",
    "    extracted_transcripts = list(\n",
    "        extract_transcripts(\n",
    "            filter_links(merge_uniqefy_links(get_transcript_links(urls)))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # sort the transcripts by episode title\n",
    "    sorted_transcripts = sorted(extracted_transcripts, key=lambda x: x[\"title\"])\n",
    "\n",
    "    # pickle the transcripts\n",
    "    with open(\"extracted_sg1_transcripts.pickle\", \"wb\") as f:\n",
    "        pickle.dump(sorted_transcripts, f)\n",
    "\n",
    "    # transcripts is a list of dicts that include other metadata\n",
    "    # with the key \"text\" containing the actual transcript\n",
    "    return sorted_transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "014b5ecb-9085-4954-ae4b-041ed17d78d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"http://www.stargate-sg1-solutions.com/wiki/Season_One_Transcripts\",\n",
    "    \"http://www.stargate-sg1-solutions.com/wiki/Season_Two_Transcripts\",\n",
    "    \"http://www.stargate-sg1-solutions.com/wiki/Season_Three_Transcripts\",\n",
    "    \"http://www.stargate-sg1-solutions.com/wiki/Season_Four_Transcripts\",\n",
    "    \"http://www.stargate-sg1-solutions.com/wiki/Season_Five_Transcripts\",\n",
    "    \"http://www.stargate-sg1-solutions.com/wiki/Season_Six_Transcripts\",\n",
    "    \"http://www.stargate-sg1-solutions.com/wiki/Season_Seven_Transcripts\",\n",
    "    \"http://www.stargate-sg1-solutions.com/wiki/Season_Eight_Transcripts\",\n",
    "    \"http://www.stargate-sg1-solutions.com/wiki/Season_Nine_Transcripts\",\n",
    "    \"http://www.stargate-sg1-solutions.com/wiki/Season_Ten_Transcripts\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b26d6df5-2850-4e54-a000-09596163b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = run_extraction_pipeline(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53cd8ae-6d73-4b70-87c7-448ae5e09029",
   "metadata": {},
   "source": [
    "## Step 2: Preprocess and Build Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4098436e-7623-46d1-8e51-c81ea4c236ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(corpus: Iterator[str]) -> Iterator[str]:\n",
    "    pattern_1 = re.compile(r\"(TEASER|FADE\\s(IN|OUT))\")\n",
    "    pattern_2 = re.compile(r\"((END|ROLL)\\sCREDIT).*\")\n",
    "    for doc in corpus:\n",
    "        doc = re.sub(pattern_2, \"\", re.sub(pattern_1, \"\", doc)).strip()\n",
    "        yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1877961b-5785-4573-9668-94775d192544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load transcripts\n",
    "with open(\"extracted_sg1_transcripts.pickle\", \"rb\") as f:\n",
    "    transcripts = pickle.load(f)\n",
    "\n",
    "corpus = (t[\"text\"] for t in transcripts)\n",
    "cleaned_corpus = list(clean_corpus(corpus))\n",
    "\n",
    "# pickle the cleaned corpus\n",
    "with open(\"cleaned_sg1_corpus.pickle\", \"wb\") as f:\n",
    "    pickle.dump(cleaned_corpus, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7508429-55fe-4ea2-af21-784b5b7bd047",
   "metadata": {},
   "source": [
    "## Step 3: Create Dictionary, Tokenize and build Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4823efd7-d102-4401-a08a-2b16a66d680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the cleaned corpus\n",
    "with open(\"cleaned_sg1_corpus.pickle\", \"rb\") as f:\n",
    "    corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53aa8c7d-4a2b-4c62-abad-c044fed01a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenize(docs: list[str]) -> Iterator[list]:\n",
    "    \"\"\"\n",
    "    Input: docs: list of sentences\n",
    "    Output: generator or list of lists of tokens\n",
    "    \"\"\"\n",
    "    cleaned = (\n",
    "        gensim.parsing.preprocessing.strip_multiple_whitespaces(\n",
    "            gensim.parsing.preprocessing.strip_non_alphanum(gensim.utils.deaccent(doc))\n",
    "        )\n",
    "        .strip()\n",
    "        .lower()\n",
    "        for doc in docs\n",
    "    )\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_lg\", exclude=[\"parser\", \"ner\", \"tok2vec\"])\n",
    "\n",
    "    # all the documents (rows) in the corpus\n",
    "    sents = nlp.pipe(cleaned, n_process=6)\n",
    "\n",
    "    # only keep the lemma form of the token and if token is alphabetic and not a stopword\n",
    "    def ok_token(tok):\n",
    "        if not tok.is_stop and len(tok) > 1 and not tok.like_num:\n",
    "            return tok.lemma_\n",
    "        else:\n",
    "            return\n",
    "\n",
    "    # iterate through each document and each token\n",
    "    # return list of cleaned (lemmatized) strings\n",
    "    # res = (ok_token(token) for sent in sents for token in sent if ok_token(token) != None)\n",
    "    res = (\n",
    "        [ok_token(token) for token in sent if ok_token(token) != None] for sent in sents\n",
    "    )\n",
    "\n",
    "    # clean up\n",
    "    del docs\n",
    "    del nlp\n",
    "    del cleaned\n",
    "    return res\n",
    "\n",
    "\n",
    "def build_bow_corpus(cleaned_dataset: list[list[str]], dictionary) -> Iterator[list]:\n",
    "    \"\"\"\n",
    "    Input: - cleaned_dataset: list of list of words\n",
    "           - dictionary: gensim dictionary object\n",
    "    Output: - generator of list of bags of words\n",
    "    \"\"\"\n",
    "    for doc in cleaned_dataset:\n",
    "        yield dictionary.doc2bow(doc, allow_update=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8411af73-3921-42ea-be95-94584c74f5b5",
   "metadata": {},
   "source": [
    "## Step 4: Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd60e168-4f8d-42e7-acdd-7bdc4ec95ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_topics(corpus, id2word, sentences, n_topics: int):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - corpus: list / generator of bags of words\n",
    "        - id2word: gensim dictionary object\n",
    "        - sentences: list / generator of tokens\n",
    "        - n_topics: int number of topics\n",
    "    Output:\n",
    "        - Coherence value (float)\n",
    "    \"\"\"\n",
    "    cores = 7\n",
    "    lda_model = gensim.models.LdaMulticore(\n",
    "        corpus=list(corpus), id2word=id2word, num_topics=n_topics, workers=cores\n",
    "    )\n",
    "    chr_model = gensim.models.CoherenceModel(\n",
    "        model=lda_model,\n",
    "        texts=sentences,\n",
    "        dictionary=id2word,\n",
    "        coherence=\"c_v\",\n",
    "        processes=cores,\n",
    "    )\n",
    "    return chr_model.get_coherence(), lda_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9f48b4-bb92-436a-887b-0a966a9fe137",
   "metadata": {},
   "source": [
    "## Step 5: Insights / Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4b889f-af3e-4f2d-8756-b8df15d640c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa31cb-e658-4390-aa05-358c148d024e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baa22f0-4a02-4a97-8106-7c36e5ecda48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
